{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgF0GKC0aUPVY8TMfvD0sS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB2iMN6vd-G8","executionInfo":{"status":"ok","timestamp":1695085143888,"user_tz":-120,"elapsed":1067915,"user":{"displayName":"Ma So","userId":"08933669274528398728"}},"outputId":"b5b9a589-037f-4cf7-b6c4-eece60f999ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.2 multiprocess-0.70.15 xxhash-3.3.0\n","Collecting sklearn\n","  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 48784/48784 [00:35<00:00, 1362.87it/s]\n","100%|██████████| 2539/2539 [00:01<00:00, 2023.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["The number of components for the selected variance is: 239\n"]}],"source":["!pip install datasets\n","!pip install sklearn\n","import tensorflow as tf\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from datasets import load_dataset, load_from_disk\n","import os\n","import pickle\n","from tqdm import tqdm\n","import pandas as pd\n","# Data processing\n","import numpy as np# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns# Dataset\n","from sklearn import datasets# Dimensionality reduction\n","from sklearn.decomposition import PCA# Modeling\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.cluster import KMeans\n","import scipy.cluster.hierarchy as hier\n","from sklearn.mixture import GaussianMixture# Number of clusters\n","from sklearn.metrics import silhouette_score\n","from datasets import load_dataset, load_from_disk\n","import os\n","import pickle\n","from tqdm import tqdm\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","\n","type_to_t = {\n","    'disorder': 'T-11',\n","    'procedure': 'T-39',\n","    'finding': 'T-18',\n","    'substance': 'T-55'\n","}\n","\n","with open('drive/MyDrive/Tesi_LossAcc/id2tkn.pickle', 'rb') as handle:\n","    id2tkn = pickle.load(handle)\n","\n","data_path = 'drive/MyDrive/Tesi_LossAcc/annotations_stream_phase2_v1_1d_256_ALL_TYPES_prepared_split'\n","cui_types = 'disorder,procedure,finding,substance'\n","#cui_types = 'disorder'\n","FOLDER = \"drive/MyDrive/Tesi_LossAcc/[foresight_small]-[types={}]\".format(cui_types)\n","subset = 8000\n","\n","cui_types = cui_types.split(',')\n","def map_to_df(data_path, mode, cui_types, id2tkn):\n","    dataset = load_from_disk(os.path.join(data_path, mode))\n","    cui_types = [type_to_t[t] for t in cui_types] + ['sep']\n","\n","    df = []\n","    for sample in tqdm(dataset):\n","        patient_id = sample['patient_id']\n","        ts = 0\n","        tokens = []\n","        for token_type, token in zip(sample['token_type'], sample['input_ids']):\n","            if token_type not in cui_types: continue\n","\n","            if token_type == 'sep':\n","                ts += 1\n","            else:\n","                if token not in tokens:\n","                    df.append([patient_id, token_type, id2tkn[token], ts])\n","                    tokens.append(token)\n","    df = pd.DataFrame(df)\n","    return df\n","df_train = map_to_df(data_path, 'train', cui_types, id2tkn)\n","df_test = map_to_df(data_path, 'test', cui_types, id2tkn)\n","\n","cols = df_train.columns\n","df_train.rename (columns = {cols[0]:'patient_id',cols[1]:'concept_types',cols[2]:'concept',cols[3]:'timestamp'}, inplace=True)\n","cols_2 = df_test.columns\n","df_test.rename (columns = {cols_2[0]:'patient_id',cols_2[1]:'concept_types',cols_2[2]:'concept',cols_2[3]:'timestamp'}, inplace=True)\n","\n","df_train = df_train.drop_duplicates()\n","df_test = df_test.drop_duplicates()\n","\n","\n","#Remove patient with a single timestamp value\n","patient_count_train = df_train.groupby(\"patient_id\").agg({\"timestamp\": \"nunique\"})\n","patient_count_train = patient_count_train[(patient_count_train[\"timestamp\"] == 1)] #& (patient_count_t[\"concept\"] == 1)]\n","df_train = df_train[~df_train['patient_id'].isin(patient_count_train.index)]\n","\n","patient_count_t = df_test.groupby(\"patient_id\").agg({\"timestamp\": \"nunique\"})\n","patient_count_t = patient_count_t[(patient_count_t[\"timestamp\"] == 1)] #& (patient_count_t[\"concept\"] == 1)]\n","df_test = df_test[~df_test['patient_id'].isin(patient_count_t.index)]\n","\n","# Concatena df_train e df_test\n","new_df = pd.concat([df_train, df_test], ignore_index=True)\n","\"\"\"#GROUND TRUTH GENERATION\n","time = 1\n","#Creazione dataframe contenente il numero di timestamp unici per patient_id\n","df_timestamp_unic = new_df.groupby('patient_id')['timestamp'].nunique().reset_index()\n","df_timestamp_unic.columns = ['patient_id', 'timestamp_unic']\n","#Generazione ground_truth\n","ground_truth = pd.DataFrame() # creazione del DataFrame vuoto\n","for patient_id in df_timestamp_unic['patient_id'].unique():\n","    timestamp_unic = df_timestamp_unic.loc[df_timestamp_unic['patient_id'] == patient_id, 'timestamp_unic'].iloc[0]\n","    if timestamp_unic > time:\n","        group = new_df.loc[new_df['patient_id'] == patient_id] # selezione del gruppo di righe per il paziente corrente\n","        unique_timestamps = group['timestamp'].unique()\n","        max_timestamps = pd.Series(unique_timestamps).nlargest(time)\n","        ground_truth = pd.concat([ground_truth, group.loc[group['timestamp'].isin(max_timestamps)]])\n","    elif timestamp_unic <= time:\n","        group = new_df.loc[(new_df['patient_id'] == patient_id) & (new_df['timestamp'] != 0)] # selezione del gruppo di righe per il paziente corrente, escludendo i timestamp uguali a zero\n","        ground_truth = pd.concat([ground_truth, group])\n","ground_truth.reset_index(drop=True, inplace=True) # reset dell'indice del DataFrame\"\"\"\n","# Load ground truth file\n","ground_truth = pd.read_csv('/content/drive/MyDrive/Tesi_LossAcc/ground_truthTimestampDF.csv')\n","# FILTERING\n","# Remove of all concepts other than T-11 in ground truth\n","ground_truth = ground_truth[ground_truth['concept_types'] == 'T-11']\n","new_df['patient_id'] = new_df['patient_id'].astype('int64')\n","new_df['concept'] = new_df['concept'].astype('int64')\n","\n","merged_test_time = pd.merge(new_df, ground_truth, on=[\"patient_id\", \"concept\", \"concept_types\", \"timestamp\"], how=\"outer\", indicator=True)\n","filtered_test_time = merged_test_time[merged_test_time[\"_merge\"] == \"left_only\"]\n","new_df1 = filtered_test_time.drop(\"_merge\", axis=1)\n","\n","unique_sources = ground_truth['patient_id'].unique()\n","\n","new_df2 = new_df1[new_df1['patient_id'].isin(unique_sources)]\n","new_df2 = new_df2.copy()\n","new_df2['patient_id'] = new_df2['patient_id'].astype('uint32')\n","new_df2['concept'] = new_df2['concept'].astype('uint32')\n","new_df2['timestamp'] = new_df2['timestamp'].astype('uint32')\n","# Removal of previously encountered disorders\n","common_concepts = pd.merge(ground_truth, new_df2, on=['patient_id', 'concept'])\n","\n","common_patient_ids = common_concepts['patient_id'].unique()\n","common_concepts = common_concepts[['patient_id', 'concept']]\n","filtered_ground_truth = ground_truth[~((ground_truth['patient_id'].isin(common_concepts['patient_id'])) & (ground_truth['concept'].isin(common_concepts['concept'])))]\n","\n","ground_truth_filtered = ground_truth[ground_truth['patient_id'].isin(filtered_ground_truth['patient_id'])]\n","unique_sources = ground_truth_filtered['patient_id'].unique()\n","\n","new_df = new_df2[new_df2['patient_id'].isin(unique_sources)]\n","all_df = new_df\n","all_time = all_df[all_df['patient_id'].isin(unique_sources)]\n","num_all = all_time['patient_id'].nunique()\n","all_time_disorder = all_time[all_time['concept_types'] == 'T-11'].copy()\n","conteggio_occorrenze = all_time_disorder.groupby(all_time_disorder.columns.tolist(),as_index=False).size()\n","cols_prova = conteggio_occorrenze.columns\n","conteggio_occorrenze.rename(columns = {cols_prova[4]:'occurrence'}, inplace=True)\n","dis_clust = (conteggio_occorrenze.groupby('patient_id')['concept'].value_counts().unstack().fillna(0)).reset_index()\n","dis_clust2 = dis_clust.set_index('patient_id')\n","\n","# LABEL GENERATION\n","label_vect = (ground_truth.groupby('patient_id')['concept'].value_counts().unstack().fillna(0)).reset_index()\n","label_vect.rename(columns={'patient_id': 'source'}, inplace=True)\n","#label_vect.to_csv(\"/content/drive/MyDrive/Tesi_LossAcc/LabelsLess1364.csv\")\n","\n","# FEATURES GENERATION\n","feat_vect = (new_df2.groupby('patient_id')['concept'].value_counts().unstack().fillna(0)).reset_index()\n","feat_vect.rename(columns={'patient_id': 'source'}, inplace=True)\n","# feat_vect.to_csv(\"/content/drive/MyDrive/Tesi_LossAcc/FeatLessPat.csv\")\n","\n","# TRUNCATEDSVD PCA\n","X = dis_clust2.values\n","tsvd = TruncatedSVD(n_components=X.shape[1])\n","X_tsvd = tsvd.fit_transform(X)\n","\n","n_components=X.shape[1]\n","\n","variance_cumulative = np.cumsum(tsvd.explained_variance_ratio_)\n","\n","\"\"\"# Plot\n","plt.plot(variance_cumulative)\n","plt.xlabel('Numero di componenti')\n","plt.ylabel('Varianza cumulativa spiegata')\n","plt.show()\"\"\"\n","\n","transformed_data_reduced = X_tsvd[:,:n_components]\n","\n","df_pcas = {}\n","n_components_dict = {}\n","variance_percentages = [50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n","\n","for p in variance_percentages:\n","    n_components = np.argmax(variance_cumulative >= p/100) + 1\n","    n_components_dict[p] = n_components\n","    transformed_data_reduced = X_tsvd[:,:n_components]\n","    df_pca = pd.DataFrame(transformed_data_reduced, columns=[f'PC{i+1}' for i in range(n_components)])\n","    df_pca.index = dis_clust2.index\n","    df_pcas[p] = df_pca\n","\n","df_pca = df_pcas[80]\n","print(\"The number of components for the selected variance is:\",n_components_dict[80])\n","\n","#KMEANS\n","n_clusters = 2685\n","kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n","\n","df_pca_copy = df_pca.copy()\n","df_pca_copy['cluster'] = kmeans.fit_predict(df_pca_copy)\n","df_copy = dis_clust2.copy()\n","df_copy.reset_index(inplace=True)\n","df_pca_copy.reset_index(inplace=True)\n","df_pca_copy[\"patient_id\"] = df_pca_copy[\"patient_id\"].astype(int)\n","df_copy[\"patient_id\"] = df_copy[\"patient_id\"].astype(int)\n","df_copy['cluster'] = df_copy['patient_id'].map(dict(zip(df_pca_copy['patient_id'],df_pca_copy['cluster'])))\n","grouped = df_copy.groupby(\"cluster\")\n","\n","#EDGES GENERATION\n","# Count the number of occurrences of each cluster\n","cluster_counts = df_copy['cluster'].value_counts()\n","# Filter clusters with only one patient_id\n","single_patient_clusters = cluster_counts[cluster_counts == 1].index.tolist()\n","# Filter the dataframe to include only the single patient clusters\n","df_single_patient_clusters = df_copy[df_copy['cluster'].isin(single_patient_clusters)]\n","df_primiEdge = df_copy[['patient_id', 'cluster']].copy()\n","merge_clust = df_primiEdge.copy()\n","merge_clust = merge_clust.merge(df_primiEdge, on='cluster')\n","merge_clust.drop_duplicates(inplace=True)\n","#Removing self-connection\n","merge_clust = merge_clust[merge_clust['patient_id_x'] != merge_clust['patient_id_y']]\n","edge_cluster = merge_clust\n","# Replace the 'cluster' column with the 'label' column and assign the value 'has_common'\n","edge_cluster['label'] = edge_cluster['cluster'].map(lambda x: 'has_common')\n","# Remove the 'common_concept' column\n","edge_cluster = edge_cluster.drop('cluster', axis=1)\n","edge_cluster = edge_cluster.rename(columns={'patient_id_x': 'source'})\n","edge_cluster = edge_cluster.rename(columns={'patient_id_y': 'target'})\n","#edge_cluster.to_csv(\"/content/drive/MyDrive/Tesi_LossAcc/EdgeGraph_Clustered2685.csv\")"]}]}