{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONA54AgqkcvhokosTXB6kg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZhKIQOVTERp","executionInfo":{"status":"ok","timestamp":1695081651991,"user_tz":-120,"elapsed":5633430,"user":{"displayName":"Ma So","userId":"08933669274528398728"}},"outputId":"c3d08095-8161-4c7d-ea9f-164814cb3335"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.17.2 multiprocess-0.70.15 xxhash-3.3.0\n","Collecting kneed\n","  Downloading kneed-0.8.5-py3-none-any.whl (10 kB)\n","Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.23.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.11.2)\n","Installing collected packages: kneed\n","Successfully installed kneed-0.8.5\n","Collecting sklearn\n","  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post9-py3-none-any.whl size=2952 sha256=c91f174c23648f83a9b90a2820e4d63b1534445902c6745019cc1999817876d3\n","  Stored in directory: /root/.cache/pip/wheels/33/a3/d2/092b519e9522b4c91608b7dcec0dd9051fa1bff4c45f4502d1\n","Successfully built sklearn\n","Installing collected packages: sklearn\n","Successfully installed sklearn-0.0.post9\n","Mounted at /content/drive\n","Enter the percentage of variance to retain: 80\n","Enter the number of clusters to retain:  2685\n","Enter the number of timestamps to keep in the ground truth: 1\n","Enter the value of k, the number of concepts to predict:10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 48784/48784 [00:38<00:00, 1259.43it/s]\n","100%|██████████| 2539/2539 [00:01<00:00, 1822.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Which concept types do you want to EXCLUDE? (separate by comma)\n","The  concepts have been removed from the dataframes.\n","The number of components for the selected variance is:  417\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Do you want to predict only concepts of type 'disorder'? (y or n): y\n","The concepts have been removed.\n","Correct samples: 4249\n","Mean Reciprocal Rank:  0.029836579534881755\n","Samples: 1490\n","Positive samples: 245\n","True Positive Rate:  0.16442953020134227\n","Correct Predictions: 292\n","Hits:  0.01959731543624161\n","Mean Recall:  0.06340314902888926\n","Mean Average Precision:  0.06203306434430595\n"]}],"source":["!pip install datasets\n","!pip install kneed\n","!pip install sklearn\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from datasets import load_dataset, load_from_disk\n","import os\n","import pickle\n","from tqdm import tqdm\n","import pandas as pd\n","# Data processing\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets# Dimensionality reduction\n","from sklearn.decomposition import PCA# Modeling\n","from sklearn.cluster import KMeans\n","import scipy.cluster.hierarchy as hier\n","from sklearn.mixture import GaussianMixture# Number of clusters\n","from sklearn.metrics import silhouette_score\n","\n","def get_user_input():\n","    per_var = input(\"Enter the percentage of variance to retain: \")\n","    n_clusters = input(\"Enter the number of clusters to retain:  \")\n","    time = input(\"Enter the number of timestamps to keep in the ground truth: \")\n","    k = input(\"Enter the value of k, the number of concepts to predict:\")\n","\n","    return int(per_var), int(n_clusters), int(time), int(k)\n","\n","per_var, n_clusters, time, k = get_user_input()\n","type_to_t = {\n","    'disorder': 'T-11',\n","    'procedure': 'T-39',\n","    'finding': 'T-18',\n","    'substance': 'T-55'\n","}\n","\n","with open('drive/MyDrive/Tesi_LossAcc/id2tkn.pickle', 'rb') as handle:\n","    id2tkn = pickle.load(handle)\n","\n","data_path = 'drive/MyDrive/Tesi_LossAcc/annotations_stream_phase2_v1_1d_256_ALL_TYPES_prepared_split'\n","cui_types = 'disorder,procedure,finding,substance'\n","#cui_types = 'disorder'\n","FOLDER = \"drive/MyDrive/Tesi_LossAcc/[foresight_small]-[types={}]\".format(cui_types)\n","subset = 8000\n","\n","cui_types = cui_types.split(',')\n","def map_to_df(data_path, mode, cui_types, id2tkn):\n","    dataset = load_from_disk(os.path.join(data_path, mode))\n","    cui_types = [type_to_t[t] for t in cui_types] + ['sep']\n","\n","    df = []\n","    for sample in tqdm(dataset):\n","        patient_id = sample['patient_id']\n","        ts = 0\n","        tokens = []\n","        for token_type, token in zip(sample['token_type'], sample['input_ids']):\n","            if token_type not in cui_types: continue\n","\n","            if token_type == 'sep':\n","                ts += 1\n","            else:\n","                if token not in tokens:\n","                    df.append([patient_id, token_type, id2tkn[token], ts])\n","                    tokens.append(token)\n","    df = pd.DataFrame(df)\n","    return df\n","\n","df_train = map_to_df(data_path, 'train', cui_types, id2tkn)\n","df_test = map_to_df(data_path, 'test', cui_types, id2tkn)\n","\n","cols = df_train.columns\n","df_train.rename (columns = {cols[0]:'patient_id',cols[1]:'concept_types',cols[2]:'concept',cols[3]:'timestamp'}, inplace=True)\n","cols_2 = df_test.columns\n","df_test.rename (columns = {cols_2[0]:'patient_id',cols_2[1]:'concept_types',cols_2[2]:'concept',cols_2[3]:'timestamp'}, inplace=True)\n","duplicate = df_train[df_train.duplicated()]\n","df_train = df_train.drop_duplicates()\n","df_test = df_test.drop_duplicates()\n","df_train = df_train.drop(df_train.columns[[3]], axis=1)\n","conteggio_occorrenze = df_train.groupby(df_train.columns.tolist(),as_index=False).size()\n","cols_prova = conteggio_occorrenze.columns\n","conteggio_occorrenze.rename(columns = {cols_prova[3]:'occurrence'}, inplace=True)\n","\n","def filter_concept_types(df_train, df_test):\n","    concept_type_mapping = {'T-11': 'disorder', 'T-39': 'procedure', 'T-18': 'finding', 'T-55': 'substance'}\n","    excluded_concept_types = input(\"Which concept types do you want to EXCLUDE? (separate by comma)\").split(',')\n","    excluded_concept_types = [concept_type_mapping.get(x.strip(), x.strip()) for x in excluded_concept_types]\n","    excluded_concept_types = set(excluded_concept_types)\n","\n","    for df in [df_train, df_test]:\n","        df.drop(df[df['concept_types'].apply(lambda x: concept_type_mapping.get(x, x)) \\\n","                    .isin(excluded_concept_types)].index, inplace=True)\n","\n","    print(\"The {} concepts have been removed from the dataframes.\".format(', '.join(excluded_concept_types)))\n","\n","def filter_concept_types2(ground_truth):\n","    concept_type_mapping = {'T-11': 'disorder', 'T-39': 'procedure', 'T-18': 'finding', 'T-55': 'substance'}\n","    excluded_concept_types = set()\n","\n","    user_response = input(\"Do you want to predict only concepts of type 'disorder'? (y or n): \")\n","\n","    if user_response.lower() == 'y':\n","        excluded_concept_types.add('T-39')\n","        excluded_concept_types.add('procedure')\n","        excluded_concept_types.add('T-18')\n","        excluded_concept_types.add('finding')\n","        excluded_concept_types.add('T-55')\n","        excluded_concept_types.add('substance')\n","\n","        ground_truth.drop(ground_truth[ground_truth['concept_types'].apply(lambda x: concept_type_mapping.get(x, x)) \\\n","                                      .isin(excluded_concept_types)].index, inplace=True)\n","\n","\n","        print(\"The concepts have been removed.\")\n","    elif user_response.lower() == 'n':\n","        print(\"The concepts have not been removed.\")\n","    else:\n","        print(\"Invalid response. No changes made.\")\n","\n","    return user_response, ground_truth\n","\n","filter_concept_types(df_train, df_test)\n","\n","df_train = conteggio_occorrenze\n","\n","df_train_R = df_train.groupby(['patient_id', 'concept', 'concept_types'])['occurrence'].sum().reset_index()\n","df_train_nR = pd.crosstab(index=df_train['patient_id'], columns=df_train['concept'], values=df_train['occurrence'], aggfunc='sum', dropna=True)\n","df_train_nR.fillna(0, inplace=True)\n","df_addestramento = df_train_nR\n","\n","#truncated SVD\n","import numpy as np\n","from sklearn.decomposition import TruncatedSVD\n","import matplotlib.pyplot as plt\n","\n","X = df_addestramento.values\n","tsvd = TruncatedSVD(n_components=X.shape[1])\n","X_tsvd = tsvd.fit_transform(X)\n","\n","n_components=X.shape[1]\n","\n","variance_cumulative = np.cumsum(tsvd.explained_variance_ratio_)\n","transformed_data_reduced = X_tsvd[:,:n_components]\n","\n","df_pcas = {}\n","n_components_dict = {}\n","variance_percentages = [50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n","\n","for p in variance_percentages:\n","    n_components = np.argmax(variance_cumulative >= p/100) + 1\n","    n_components_dict[p] = n_components\n","    transformed_data_reduced = X_tsvd[:,:n_components]\n","    df_pca = pd.DataFrame(transformed_data_reduced, columns=[f'PC{i+1}' for i in range(n_components)])\n","    df_pca.index = df_addestramento.index\n","    df_pcas[p] = df_pca\n","\n","df_pca = df_pcas[per_var]\n","print(\"The number of components for the selected variance is: \",n_components_dict[per_var])\n","\n","#KMEANS\n","kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n","df_pca_copy = df_pca.copy()\n","df_pca_copy['cluster'] = kmeans.fit_predict(df_pca_copy)\n","df_copy = df_addestramento.copy()\n","df_copy.reset_index(inplace=True)\n","df_pca_copy.reset_index(inplace=True)\n","df_pca_copy[\"patient_id\"] = df_pca_copy[\"patient_id\"].astype(int)\n","df_copy[\"patient_id\"] = df_copy[\"patient_id\"].astype(int)\n","df_copy['cluster'] = df_copy['patient_id'].map(dict(zip(df_pca_copy['patient_id'],df_pca_copy['cluster'])))\n","grouped = df_copy.groupby(\"cluster\")\n","\n","\"\"\"# per ogni cluster, stampare i 10 concetti con occorrenza più elevata ed il relativo numero di occorrenze\n","for name, group in grouped:\n","    print(\"Cluster:\", name)\n","    count = group.drop(columns=['cluster','patient_id']).reset_index(drop=True).sum().sort_values(ascending=False)\n","    top_10 = count.head(10)\n","    #print(top_10)\n","    #print()\"\"\"\n","df_trained = df_pca_copy\n","patient_count_t = df_test.groupby(\"patient_id\").agg({\"timestamp\": \"nunique\"})\n","patient_count_t = patient_count_t[(patient_count_t[\"timestamp\"] == 1)] #& (patient_count_t[\"concept\"] == 1)]\n","\n","df_test = df_test[~df_test['patient_id'].isin(patient_count_t.index)]\n","df_timestamp_unic = df_test.groupby('patient_id')['timestamp'].nunique().reset_index()\n","df_timestamp_unic.columns = ['patient_id', 'timestamp_unic']\n","\n","#GROUND TRUTH\n","ground_truth = pd.DataFrame() # creazione del DataFrame vuoto\n","for patient_id in df_timestamp_unic['patient_id'].unique():\n","    timestamp_unic = df_timestamp_unic.loc[df_timestamp_unic['patient_id'] == patient_id, 'timestamp_unic'].iloc[0]\n","    if timestamp_unic > time:\n","        group = df_test.loc[df_test['patient_id'] == patient_id] # selezione del gruppo di righe per il paziente corrente\n","        unique_timestamps = group['timestamp'].unique()\n","        max_timestamps = pd.Series(unique_timestamps).nlargest(time)\n","        ground_truth = pd.concat([ground_truth, group.loc[group['timestamp'].isin(max_timestamps)]])\n","    elif timestamp_unic <= time:\n","        group = df_test.loc[(df_test['patient_id'] == patient_id) & (df_test['timestamp'] != 0)] # selezione del gruppo di righe per il paziente corrente, escludendo i timestamp uguali a zero\n","        ground_truth = pd.concat([ground_truth, group])\n","ground_truth.reset_index(drop=True, inplace=True) # reset dell'indice del DataFrame\n","\n","user_response, ground_trurh = filter_concept_types2(ground_truth)\n","merged_test_time = pd.merge(df_test, ground_truth, on=[\"patient_id\", \"concept\", \"concept_types\", \"timestamp\"], how=\"outer\", indicator=True)\n","filtered_test_time = merged_test_time[merged_test_time[\"_merge\"] == \"left_only\"]\n","df_test1 = filtered_test_time.drop(\"_merge\", axis=1)\n","df_test_concept = (df_test1.groupby('patient_id')['concept'].value_counts().unstack().fillna(0))\n","df_result = pd.DataFrame(0, index=df_test_concept.index, columns=df_addestramento.columns)\n","df_result.update(df_test_concept)\n","\n","#PCA on test data\n","X_test = df_result.values\n","pca_test = PCA(n_components=n_components_dict[per_var])\n","transformed_data_reduced_test = pca_test.fit_transform(X_test)\n","df_pca_test = pd.DataFrame(transformed_data_reduced_test, columns=[f'PC{i+1}' for i in range(pca_test.n_components_)])\n","df_pca_test.index = df_result.index\n","\n","# KMEANS on test data\n","# PREDICTION\n","df_pca_test['cluster'] = kmeans.predict(df_pca_test)\n","df_results_copy = df_result.copy()\n","df_results_copy['cluster'] = df_pca_test['cluster']\n","df_GTT1 = ground_truth.groupby('patient_id')['concept'].apply(list).reset_index(name='concepts')\n","df_GTT1 = df_GTT1.explode('concepts')\n","df_GTT1 = pd.concat([df_GTT1.drop(columns='concepts'), df_GTT1['concepts'].apply(pd.Series)], axis=1)\n","df_addestrato = df_copy\n","df_grouped = df_addestrato.groupby('cluster').sum()\n","columns = df_grouped.columns\n","columns = [col for col in columns if col not in ['patient_id', 'cluster']]\n","for i in range(0, n_clusters):\n","    df_cluster = df_grouped.loc[i, columns]\n","    concepts = df_cluster[df_cluster > 0].index.tolist()\n","dfs = []\n","\n","for cluster, group in grouped:\n","\n","    for i, row in group.iterrows():\n","        patient_id = row['patient_id']\n","        concepts = row.iloc[1:-1]\n","        concepts = concepts[concepts != 0]\n","        df_concepts = pd.DataFrame({'cluster': [cluster] * len(concepts),\n","                                    'patient_id': [patient_id] * len(concepts),\n","                                    'concepts': concepts.index,\n","                                    'occurence': concepts.values})\n","        dfs.append(df_concepts)\n","df_concepts = pd.concat(dfs, ignore_index=True)\n","#Creating dataframes to calculate metrics\n","df_concepts = df_concepts.sort_values(by=['cluster', 'patient_id'])\n","df_concepts_grouped = df_concepts.groupby(['cluster', 'concepts'])['occurence'].sum().reset_index()\n","df_concepts_grouped = df_concepts_grouped[['cluster', 'concepts', 'occurence']]\n","df_concepts_grouped = df_concepts_grouped.sort_values(by=['cluster', 'occurence'], ascending=[True, False])\n","df_concepts_ranked = df_concepts_grouped.groupby('cluster').apply(lambda x: x.sort_values('occurence', ascending=False))\n","df_concepts_ranked = df_concepts_ranked.reset_index(drop=True)\n","df_concepts_ranked['rank'] = df_concepts_ranked.groupby('cluster')['occurence'].rank(method='dense', ascending=False)\n","df_concepts_ranked = df_concepts_ranked.rename(columns={'concepts': 'concept'})\n","df_prediction = df_results_copy.reset_index().loc[:, [\"patient_id\", \"cluster\"]]\n","df_prediction = df_prediction.reset_index(drop=True)\n","df_merged = pd.merge(df_prediction, df_GTT1, on='patient_id')\n","df_merged = df_merged.rename(columns={0: 'concept'})\n","df_merged['cluster'] = df_merged['cluster'].astype(np.int64)\n","df_prova1 = pd.merge(df_merged, df_concepts_ranked, on=['concept', 'cluster'], how='left')\n","df_prova1.loc[df_prova1['rank'].isna(), 'rank'] = np.nan\n","df_prova1.fillna(0, inplace=True)\n","\n","\n","df_types = df_train.drop(columns = ['patient_id','occurrence'])\n","\n","if user_response.lower() == 'y':\n","  batch_size = 10000\n","  num_batches = len(df_concepts_ranked) // batch_size + 1\n","  df_merge_types = pd.DataFrame()\n","  for i in range(num_batches):\n","    start_idx = i * batch_size\n","    end_idx = (i + 1) * batch_size\n","    batch_concepts_ranked = df_concepts_ranked[start_idx:end_idx]\n","    batch_merge_types = batch_concepts_ranked.merge(df_types, on=\"concept\")\n","    batch_merge_types = batch_merge_types.loc[batch_merge_types[\"concept_types\"] == \"T-11\"]\n","    batch_merge_types = batch_merge_types.drop_duplicates()\n","    df_merge_types = pd.concat([df_merge_types, batch_merge_types])\n","  df_merge_types.reset_index(drop=True, inplace=True)\n","  df_merge_types.drop_duplicates()\n","  df_topk = df_merge_types\n","else:\n","  df_topk = df_concepts_ranked.copy()\n","def update_rank2(df_topk, k):\n","    updated_rows = []\n","\n","    for cluster in df_topk['cluster'].unique():\n","        cluster_data = df_topk[df_topk['cluster'] == cluster].sort_values(by='rank')\n","        selected_rows = cluster_data.head(k)\n","        updated_rows.append(selected_rows)\n","\n","    df_topk_updated = pd.concat(updated_rows)\n","\n","    return df_topk_updated\n","df_MRR = df_topk\n","df_topk_updated = update_rank2(df_topk, k)\n","\n","unique_clusters = df_topk_updated['cluster'].unique()\n","unique_concepts = df_topk_updated['concept'].unique()\n","df_topk_concepts = pd.DataFrame(0, index=unique_clusters, columns=unique_concepts)\n","for _, row in df_topk_updated.iterrows():\n","    cluster = row['cluster']\n","    concept = row['concept']\n","    df_topk_concepts.loc[cluster, concept] = 1\n","\n","df_topk_concepts = df_topk_concepts.reset_index().rename(columns={'index': 'cluster'})\n","\n","df_original_concept = df_addestramento\n","order_concept = pd.DataFrame(columns=df_original_concept.columns[:2637])\n","\n","order_concept['cluster'] = df_topk_concepts['cluster']\n","common_columns = set(order_concept.columns) & set(df_topk_concepts.columns)\n","\n","for column in common_columns:\n","    order_concept[column] = df_topk_concepts[column]\n","\n","order_concept = order_concept.fillna(0)\n","ground_truth2 = ground_truth.drop(ground_truth.columns[[3]], axis=1)\n","ground_truth_conteggio = ground_truth2.groupby(ground_truth2.columns.tolist(),as_index=False).size()\n","ground_truth_cols = ground_truth_conteggio.columns\n","ground_truth_conteggio.rename(columns = {ground_truth_cols[3]:'occurrence'}, inplace=True)\n","gt2 = pd.crosstab(index=ground_truth_conteggio['patient_id'], columns=ground_truth_conteggio['concept'], values=ground_truth_conteggio['occurrence'], aggfunc='sum', dropna=True)\n","gt2.fillna(0, inplace=True)\n","gt2 = gt2.reset_index().rename(columns={'index': 'patient_id'})\n","order_concept2 = pd.DataFrame(columns=df_original_concept.columns[:2637])\n","order_concept2['patient_id'] = gt2['patient_id']\n","common_columns = set(order_concept2.columns) & set(gt2.columns)\n","\n","for column in common_columns:\n","    order_concept2[column] = gt2[column]\n","\n","order_concept2 = order_concept2.fillna(0)\n","ranking_df = pd.crosstab(index=df_topk_updated['cluster'], columns=df_topk_updated['concept'], values=df_topk_updated['rank'], aggfunc='sum', dropna=True)\n","ranking_df_MRR = pd.crosstab(index=df_MRR['cluster'], columns=df_MRR['concept'], values=df_MRR['rank'], aggfunc='sum', dropna=True)\n","ranking_df.fillna(0, inplace=True)\n","ranking_df_MRR.fillna(0, inplace=True)\n","ranking_df = ranking_df.reset_index().rename(columns={'index': 'cluster'})\n","ranking_df_MRR = ranking_df_MRR.reset_index().rename(columns={'index': 'cluster'})\n","order_concept_MRR = pd.DataFrame(columns=df_original_concept.columns[:2637])\n","order_concept_MRR['cluster'] = ranking_df_MRR['cluster']\n","common_columns = set(order_concept_MRR.columns) & set(ranking_df_MRR.columns)\n","\n","for column in common_columns:\n","    order_concept_MRR[column] = ranking_df_MRR[column]\n","\n","order_concept_MRR = order_concept_MRR.fillna(0)\n","order_concept3 = pd.DataFrame(columns=df_original_concept.columns[:2637])\n","order_concept3['cluster'] = ranking_df['cluster']\n","common_columns = set(order_concept3.columns) & set(ranking_df.columns)\n","\n","for column in common_columns:\n","    order_concept3[column] = ranking_df[column]\n","\n","order_concept3 = order_concept3.fillna(0)\n","score_df = pd.crosstab(index=df_topk_updated['cluster'], columns=df_topk_updated['concept'], values=df_topk_updated['occurence'], aggfunc='sum', dropna=True)\n","score_df.fillna(0, inplace=True)\n","score_df = score_df.reset_index().rename(columns={'index': 'cluster'})\n","order_concept4 = pd.DataFrame(columns=df_original_concept.columns[:2637])\n","order_concept4['cluster'] = score_df['cluster']\n","common_columns = set(order_concept4.columns) & set(score_df.columns)\n","\n","for column in common_columns:\n","    order_concept4[column] = score_df[column]\n","\n","order_concept4 = order_concept4.fillna(0)\n","prev_vect = pd.DataFrame()\n","prev_vect['cluster'] = order_concept['cluster']\n","prev_values = order_concept.iloc[:, :2637].values.tolist()\n","prev_vect['prev'] = [np.array(row) for row in prev_values]\n","#LABEL\n","label_vect = pd.DataFrame()\n","label_vect['patient_id'] = order_concept2['patient_id']\n","label_values = order_concept2.iloc[:, :2637].values.tolist()\n","label_vect['label'] = [np.array(row) for row in label_values]\n","#SCORE\n","score_vect = pd.DataFrame()\n","score_vect['cluster'] = order_concept4['cluster']\n","score_values = order_concept4.iloc[:, :2637].values.tolist()\n","score_vect['score'] = [np.array(row) for row in score_values]\n","#RANK\n","rank_vect = pd.DataFrame()\n","rank_vect['cluster'] = order_concept3['cluster']\n","rank_values = order_concept3.iloc[:, :2637].values.tolist()\n","rank_vect['rank'] = [np.array(row) for row in rank_values]\n","#MRR RANK\n","rank_vect_MRR = pd.DataFrame()\n","rank_vect_MRR['cluster'] = order_concept_MRR['cluster']\n","rank_values_MRR = order_concept_MRR.iloc[:, :2637].values.tolist()\n","rank_vect_MRR['rank_MRR'] = [np.array(row) for row in rank_values_MRR]\n","\n","metriche_df = df_prova1[['patient_id', 'cluster']].drop_duplicates()\n","metriche_df[\"patient_id\"] = metriche_df[\"patient_id\"].astype('int64')\n","label_vect[\"patient_id\"] = label_vect[\"patient_id\"].astype('int64')\n","df_final = pd.merge(metriche_df, label_vect, on='patient_id', how='left')\n","df_final = pd.merge(df_final, rank_vect, on='cluster', how='left')\n","df_final = pd.merge(df_final, rank_vect_MRR, on='cluster', how='left')\n","df_final = pd.merge(df_final, prev_vect, on='cluster', how='left')\n","df_final = pd.merge(df_final, score_vect, on='cluster', how='left')\n","df_final['label'] = df_final['label'].apply(lambda x: x.astype(float))\n","df_final['prev'] = df_final['prev'].apply(lambda x: x.astype(float))\n","merged_df = df_final.copy()\n","merged_df['predicted_vector_ones'] = merged_df['prev'].apply(lambda vec: sum(x == 1 for x in vec))\n","merged_df['label_ones'] = merged_df['label'].apply(lambda vec: sum(x == 1 for x in vec))\n","merged_df['common_ones'] = merged_df.apply(lambda row: sum(x == 1 for x, y in zip(row['prev'], row['label']) if y == 1), axis=1)\n","\n","#MRR\n","def calculate_reciprocal_rank(node_id):\n","  label_vector = merged_df.loc[merged_df['patient_id'] == node_id, 'label'].values[0]\n","  ranked_vector = merged_df.loc[merged_df['patient_id'] == node_id, 'rank_MRR'].values[0]\n","\n","  positions = [i for i, val in enumerate(label_vector) if val == 1]\n","  values = [ranked_vector[i] for i in positions]\n","\n","  reciprocal_values = [1/val for val in values if val != 0]\n","  reciprocal_rank = sum(reciprocal_values)\n","\n","  return reciprocal_rank\n","merged_df['reciprocal_rank'] = merged_df['patient_id'].apply(calculate_reciprocal_rank)\n","num_camp = merged_df['label_ones'].sum()\n","print(\"Correct samples:\", num_camp)\n","reciprocalN_True = 1/num_camp\n","reciprocalRank = merged_df['reciprocal_rank'].sum()\n","MRR = reciprocalN_True*reciprocalRank\n","mrr_metric = MRR\n","print(\"Mean Reciprocal Rank: \", MRR)\n","\n","#TPR\n","num_camp = merged_df['patient_id'].nunique()\n","print(\"Samples:\", num_camp)\n","reciprocalN = 1/num_camp\n","merged_df[\"True_positive\"] = merged_df[\"common_ones\"].apply(lambda x: 1 if x > 0 else 0)\n","positive_sum = merged_df['True_positive'].sum()\n","print(\"Positive samples:\", positive_sum)\n","True_PRate = reciprocalN*positive_sum\n","TP_Metr = True_PRate\n","print(\"True Positive Rate: \", True_PRate)\n","\n","#Hits\n","num_veri_positivi = merged_df['common_ones'].sum()\n","print(\"Correct Predictions:\", num_veri_positivi)\n","hits2 = num_veri_positivi/k\n","#print(\"Rapporto tra positivi e k:\", hits2)\n","Hits = reciprocalN * hits2\n","hits = Hits\n","print(\"Hits: \", Hits)\n","\n","#Mean Recall\n","merged_df[\"False_Negative\"] = merged_df['label_ones'] - merged_df[\"common_ones\"]\n","merged_df[\"Recall\"] = merged_df['common_ones']/(merged_df['common_ones']+merged_df['False_Negative'])\n","Sum_Recall = merged_df['Recall'].sum()\n","MR = reciprocalN*Sum_Recall\n","mean_rec = MR\n","print(\"Mean Recall: \", MR)\n","\n","\n","def calculate_AP_at_k(row, k):\n","    temp_df = pd.DataFrame({\n","        'label': row['label'],\n","        'score': row['score']\n","    })\n","    temp_df = temp_df.sort_values(by='score', ascending=False)\n","    top_k = temp_df.head(k)\n","    hits = 0\n","    sum_precs = 0\n","    for i, (_, row) in enumerate(top_k.iterrows(), start=1):\n","        if row['label'] == 1:\n","            hits += 1\n","            sum_precs += hits / i\n","\n","    # AP@k\n","    if hits > 0:\n","        AP_at_k = sum_precs / hits\n","    else:\n","        AP_at_k = 0\n","\n","    return AP_at_k\n","merged_df['AP'] = merged_df.apply(calculate_AP_at_k, k=k, axis=1)\n","\n","# MAP\n","N = len(merged_df)\n","MAP = merged_df['AP'].sum() / N\n","print('Mean Average Precision: ', MAP)"]},{"cell_type":"code","source":[],"metadata":{"id":"bh-bvna6V4hu"},"execution_count":null,"outputs":[]}]}